{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6499ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8597a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'BATCH_SIZE': 4096,\n",
    "    'EPOCHS': 10,\n",
    "    'LEARNING_RATE': 1e-3,\n",
    "    'SEED' : 42\n",
    "}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ccf3735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9763b99d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1324\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyarrow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 데이터 로드\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m all_train = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./train.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyarrow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m test = pd.read_parquet(\u001b[33m\"\u001b[39m\u001b[33m./test.parquet\u001b[39m\u001b[33m\"\u001b[39m, engine=\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m).drop(columns=[\u001b[33m'\u001b[39m\u001b[33mID\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrain shape:\u001b[39m\u001b[33m\"\u001b[39m, all_train.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parquet.py:653\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;129m@doc\u001b[39m(storage_options=_shared_docs[\u001b[33m\"\u001b[39m\u001b[33mstorage_options\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_parquet\u001b[39m(\n\u001b[32m    502\u001b[39m     path: FilePath | ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    510\u001b[39m     **kwargs,\n\u001b[32m    511\u001b[39m ) -> DataFrame:\n\u001b[32m    512\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[33;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[32m    514\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    650\u001b[39m \u001b[33;03m    1    4    9\u001b[39;00m\n\u001b[32m    651\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default:\n\u001b[32m    656\u001b[39m         msg = (\n\u001b[32m    657\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe argument \u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_nullable_dtypes\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will be removed \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    658\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33min a future version.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    659\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parquet.py:79\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     69\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to find a usable engine; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtried using: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     75\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m     )\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPyArrowImpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m FastParquetImpl()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parquet.py:164\u001b[39m, in \u001b[36mPyArrowImpl.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyarrow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyarrow is required for parquet support.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\compat\\_optional.py:138\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow."
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "all_train = pd.read_parquet(\"./train.parquet\", engine=\"pyarrow\")\n",
    "test = pd.read_parquet(\"./test.parquet\", engine=\"pyarrow\").drop(columns=['ID'])\n",
    "\n",
    "print(\"Train shape:\", all_train.shape)\n",
    "print(\"Test shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6af8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicked == 1 데이터\n",
    "clicked_1 = all_train[all_train['clicked'] == 1]\n",
    "\n",
    "# clicked == 0 데이터에서 동일 개수x2 만큼 무작위 추출 (다운 샘플링)\n",
    "clicked_0 = all_train[all_train['clicked'] == 0].sample(n=len(clicked_1)*2, random_state=42)\n",
    "\n",
    "# 두 데이터프레임 합치기\n",
    "train = pd.concat([clicked_1, clicked_0], axis=0).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Train clicked:0:\", train[train['clicked']==0].shape)\n",
    "print(\"Train clicked:1:\", train[train['clicked']==1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c6a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target / Sequence\n",
    "target_col = \"clicked\"\n",
    "seq_col = \"seq\"\n",
    "\n",
    "# 학습에 사용할 피처: ID/seq/target 제외, 나머지 전부\n",
    "FEATURE_EXCLUDE = {target_col, seq_col, \"ID\"}\n",
    "feature_cols = [c for c in train.columns if c not in FEATURE_EXCLUDE]\n",
    "\n",
    "print(\"Num features:\", len(feature_cols))\n",
    "print(\"Sequence:\", seq_col)\n",
    "print(\"Target:\", target_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41745256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClickDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, seq_col, target_col=None, has_target=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.feature_cols = feature_cols\n",
    "        self.seq_col = seq_col\n",
    "        self.target_col = target_col\n",
    "        self.has_target = has_target\n",
    "\n",
    "        # 비-시퀀스 피처: 전부 연속값으로\n",
    "        self.X = self.df[self.feature_cols].astype(float).fillna(0).values\n",
    "\n",
    "        # 시퀀스: 문자열 그대로 보관 (lazy 파싱)\n",
    "        self.seq_strings = self.df[self.seq_col].astype(str).values\n",
    "\n",
    "        if self.has_target:\n",
    "            self.y = self.df[self.target_col].astype(np.float32).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X[idx], dtype=torch.float)\n",
    "\n",
    "        # 전체 시퀀스 사용 (빈 시퀀스만 방어)\n",
    "        s = self.seq_strings[idx]\n",
    "        if s:\n",
    "            arr = np.fromstring(s, sep=\",\", dtype=np.float32)\n",
    "        else:\n",
    "            arr = np.array([], dtype=np.float32)\n",
    "\n",
    "        if arr.size == 0:\n",
    "            arr = np.array([0.0], dtype=np.float32)  # 빈 시퀀스 방어\n",
    "\n",
    "        seq = torch.from_numpy(arr)  # shape (seq_len,)\n",
    "\n",
    "        if self.has_target:\n",
    "            y = torch.tensor(self.y[idx], dtype=torch.float)\n",
    "            return x, seq, y\n",
    "        else:\n",
    "            return x, seq\n",
    "\n",
    "def collate_fn_train(batch):\n",
    "    xs, seqs, ys = zip(*batch)\n",
    "    xs = torch.stack(xs)\n",
    "    ys = torch.stack(ys)\n",
    "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n",
    "    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    seq_lengths = torch.clamp(seq_lengths, min=1)  # 빈 시퀀스 방지\n",
    "    return xs, seqs_padded, seq_lengths, ys\n",
    "\n",
    "def collate_fn_infer(batch):\n",
    "    xs, seqs = zip(*batch)\n",
    "    xs = torch.stack(xs)\n",
    "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n",
    "    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    seq_lengths = torch.clamp(seq_lengths, min=1)\n",
    "    return xs, seqs_padded, seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c5585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularSeqModel(nn.Module):\n",
    "    def __init__(self, d_features, lstm_hidden=32, hidden_units=[1024, 512, 256, 128], dropout=0.2):\n",
    "        super().__init__()\n",
    "        # 모든 비-시퀀스 피처에 BN\n",
    "        self.bn_x = nn.BatchNorm1d(d_features)\n",
    "        # seq: 숫자 시퀀스 → LSTM\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=lstm_hidden, batch_first=True)\n",
    "\n",
    "        # 최종 MLP\n",
    "        input_dim = d_features + lstm_hidden\n",
    "        layers = []\n",
    "        for h in hidden_units:\n",
    "            layers += [nn.Linear(input_dim, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            input_dim = h\n",
    "        layers += [nn.Linear(input_dim, 1)]\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_feats, x_seq, seq_lengths):\n",
    "        # 비-시퀀스 피처\n",
    "        x = self.bn_x(x_feats)\n",
    "\n",
    "        # 시퀀스 → LSTM (pack)\n",
    "        x_seq = x_seq.unsqueeze(-1)  # (B, L, 1)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            x_seq, seq_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        h = h_n[-1]                  # (B, lstm_hidden)\n",
    "\n",
    "        z = torch.cat([x, h], dim=1)\n",
    "        return self.mlp(z).squeeze(1)  # logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c0951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_df, feature_cols, seq_col, target_col,\n",
    "                batch_size=512, epochs=3, lr=1e-3, device=\"cuda\"):\n",
    "\n",
    "    # 1) split\n",
    "    tr_df, va_df = train_test_split(train_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # 2) Dataset / Loader (l_max 인자 제거)\n",
    "    train_dataset = ClickDataset(tr_df, feature_cols, seq_col, target_col, has_target=True)\n",
    "    val_dataset   = ClickDataset(va_df, feature_cols, seq_col, target_col, has_target=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  collate_fn=collate_fn_train)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn_train)\n",
    "\n",
    "    # 3) 모델\n",
    "    d_features = len(feature_cols)\n",
    "    model = TabularSeqModel(d_features=d_features, lstm_hidden=64, hidden_units=[256,128], dropout=0.2).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # 4) Loop\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for xs, seqs, seq_lens, ys in tqdm(train_loader, desc=f\"Train Epoch {epoch}\"):\n",
    "            xs, seqs, seq_lens, ys = xs.to(device), seqs.to(device), seq_lens.to(device), ys.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xs, seqs, seq_lens)\n",
    "            loss = criterion(logits, ys)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * ys.size(0)\n",
    "        train_loss /= len(train_dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xs, seqs, seq_lens, ys in tqdm(val_loader, desc=f\"Val Epoch {epoch}\"):\n",
    "                xs, seqs, seq_lens, ys = xs.to(device), seqs.to(device), seq_lens.to(device), ys.to(device)\n",
    "                logits = model(xs, seqs, seq_lens)\n",
    "                loss = criterion(logits, ys)\n",
    "                val_loss += loss.item() * len(ys)\n",
    "        val_loss /= len(val_dataset)\n",
    "\n",
    "        print(f\"[Epoch {epoch}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f8013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(\n",
    "    train_df=train,\n",
    "    feature_cols=feature_cols,\n",
    "    seq_col=seq_col,\n",
    "    target_col=target_col,\n",
    "    batch_size=CFG['BATCH_SIZE'],\n",
    "    epochs=CFG['EPOCHS'],\n",
    "    lr=CFG['LEARNING_RATE'],\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412d8cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Dataset/Loader\n",
    "test_ds = ClickDataset(test, feature_cols, seq_col, has_target=False)\n",
    "test_ld = DataLoader(test_ds, batch_size=CFG['BATCH_SIZE'], shuffle=False, collate_fn=collate_fn_infer)\n",
    "\n",
    "# 2) Predict\n",
    "model.eval()\n",
    "outs = []\n",
    "with torch.no_grad():\n",
    "    for xs, seqs, lens in tqdm(test_ld, desc=\"Inference\"):\n",
    "        xs, seqs, lens = xs.to(device), seqs.to(device), lens.to(device)\n",
    "        outs.append(torch.sigmoid(model(xs, seqs, lens)).cpu())\n",
    "\n",
    "test_preds = torch.cat(outs).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471c778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['clicked'] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1f91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./baseline_submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede42018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8421510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
