# 프로젝트 요약 (2025-09-29)

## 1. 초기 분석
- `content.txt` 파일 분석을 통해 '이상신호 감지 기반 비정상 작동 진단' 대회 목표 파악.
- `smart_ocean.ipynb`의 `RandomForest` 기반 베이스라인 코드 분석.

## 2. 모델 개발 및 실험
다양한 머신러닝 모델을 사용하여 예측 성능 개선을 시도했습니다.

### 2.1. LightGBM 모델
- `RandomForest`를 대체하여 `LightGBM` 모델을 학습하고 예측을 수행했습니다.
- **스크립트**: `smart_ocean_lightGBM.py`
- **산출물**: `lightgbm_submit.csv`

### 2.2. XGBoost 모델
- 또 다른 인기 모델인 `XGBoost`를 사용하여 추가적인 예측을 수행했습니다.
- **스크립트**: `smart_ocean_xgboost.py`
- **산출물**: `xgboost_submit.csv`

### 2.3. 하이퍼파라미터 튜닝
- **GridSearchCV**: LightGBM 모델의 기본 하이퍼파라미터 튜닝을 진행했습니다.
  - **스크립트**: `smart_ocean_lightGBM_tuned.py`
  - **산출물**: `lgbm_tuned_submit.csv`
- **RandomizedSearchCV**: 더 넓은 범위의 하이퍼파라미터 탐색을 위한 스크립트를 작성했습니다. (시간 문제로 사용자가 직접 실행하기로 함)
  - **스크립트**: `lgbm_random_search.py`
  - **예상 산출물**: `lgbm_random_tuned_submit.csv`

### 2.4. 앙상블 (Ensemble)
- 4개 모델(RandomForest, LightGBM, Tuned LightGBM, XGBoost)의 예측 결과를 종합하여 최빈값으로 투표하는 앙상블을 수행했습니다.
- **스크립트**: `ensemble.py`
- **산출물**: `ensemble_submit.csv`

## 3. 모델 성능 비교
- `train.csv` 데이터에 대한 5-Fold 교차 검증(f1-score 기준)을 통해 각 모델의 성능을 객관적으로 비교했습니다.
- **스크립트**: `compare_models.py`
- **산출물**: `model_comparison.png` (성능 비교 시각화 그래프)

**교차 검증 결과:**
- **RandomForest**: 0.7714
- **LightGBM**: 0.8029
- **LightGBM_Tuned**: 0.8077
- **XGBoost**: 0.7959

## 4. 최종 산출물 목록

### 스크립트 파일
- `smart_ocean_lightGBM.py`
- `smart_ocean_lightGBM_tuned.py`
- `smart_ocean_xgboost.py`
- `lgbm_random_search.py`
- `ensemble.py`
- `compare_models.py`

### 제출 파일 (CSV)
- `baseline_submit.csv`
- `lightgbm_submit.csv`
- `lgbm_tuned_submit.csv`
- `xgboost_submit.csv`
- `ensemble_submit.csv`

### 기타 파일
- `assets/코드설명.md`
- `model_comparison.png`
- `250929.md` (현재 요약 파일)

## 5. 추가 실험 및 분석 (2025-09-30)
기존 모델의 성능을 더 끌어올리기 위해 다음과 같은 추가 실험을 진행했습니다.

### 5.1. RandomizedSearchCV 모델 성능 갱신
- `lgbm_random_search.py` 실행 결과, 교차 검증 F1 스코어 **0.8097**을 기록하여 현재까지 가장 성능이 좋은 단일 모델로 확인되었습니다.
- **산출물**: `lgbm_random_tuned_submit.csv`

### 5.2. 피처 엔지니어링
두 가지 피처 엔지니어링 전략을 시도했으나, 단독으로는 유의미한 성능 향상을 확인하지 못했습니다.
- **행(Row) 기준 통계 피처**
  - 각 데이터 샘플의 통계적 특성(평균, 표준편차 등)을 피처로 추가.
  - **스크립트**: `data_engineering.py`, `verify_fe_performance.py`
  - **결과**: F1 스코어 0.8088 (하이퍼파라미터 재튜닝 후). 기존 최고 성능(0.8097)에 미치지 못함.
- **중요 피처 기반 상호작용 피처**
  - LightGBM이 중요하다고 판단한 상위 6개 피처 간의 곱셈/나눗셈 피처를 추가.
  - **스크립트**: `interaction_feature_creation.py`
  - **결과**: F1 스코어 0.8092. 성능 향상 효과 미미.

### 5.3. 스태킹(Stacking) 앙상블
- 여러 기본 모델(RF, LGBM, XGB)의 예측 결과를 메타 모델(`LogisticRegression`)이 다시 학습하는 스태킹 앙상블을 시도했습니다.
- **스크립트**: `stacking_ensemble.py`
- **산출물**: `stacking_submit.csv`
- **결과**: 제출 결과 성능이 좋지 않아, 기본 모델의 다양성 부족 등의 원인으로 추정.

### 5.4. 데이터 분포 분석
- `target` 변수의 클래스 분포를 확인한 결과, 21개의 모든 클래스가 **완벽하게 균일한 분포**를 보이고 있음을 발견했습니다.
- **스크립트**: `check_balance.py`
- **결론**: 데이터 불균형 문제는 존재하지 않으므로, SMOTE와 같은 관련 처리 기법은 불필요합니다.

### 5.5. 현재 상황 및 다음 단계
- 현재 가장 성능이 좋은 모델은 원본 데이터에 `RandomizedSearchCV`를 적용한 LightGBM 모델(F1: 0.8097)입니다.
- 더 정밀한 하이퍼파라미터 탐색을 위해 `Optuna` 라이브러리를 사용한 `optuna_tuning.py` 스크립트를 작성했습니다.
- **다음 단계**: `optuna` 라이브러리 설치 후, `optuna_tuning.py`를 실행하여 최종 성능 향상을 시도할 예정입니다.